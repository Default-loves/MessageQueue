### 使用异步设计提升系统性能

```java

/**
 * 账户服务
 */
public interface AccountService {
    /**
     * 变更账户金额
     * @param account 账户ID
     * @param amount 增加的金额，负值为减少
     */
    CompletableFuture<Void> add(int account, int amount);
}

/**
 * 转账服务
 */
public interface TransferService {
    /**
     * 异步转账服务
     * @param fromAccount 转出账户
     * @param toAccount 转入账户
     * @param amount 转账金额，单位分
     */
    CompletableFuture<Void> transfer(int fromAccount, int toAccount, int amount);
}

/**
 * 转账服务的实现
 */
public class TransferServiceImpl implements TransferService {
    @Inject
    private  AccountService accountService; // 使用依赖注入获取账户服务的实例
    @Override
    public CompletableFuture<Void> transfer(int fromAccount, int toAccount, int amount) {
      // 异步调用add方法从fromAccount扣减相应金额
      return accountService.add(fromAccount, -1 * amount)
      // 然后调用add方法给toAccount增加相应金额
      .thenCompose(v -> accountService.add(toAccount, amount));    
    }
}

public class Client {
    @Inject
    private TransferService transferService; // 使用依赖注入获取转账服务的实例
    private final static int A = 1000;
    private final static int B = 1001;

    public void syncInvoke() throws ExecutionException, InterruptedException {
        // 同步调用
        transferService.transfer(A, B, 100).get();
        System.out.println("转账完成！");
    }

    public void asyncInvoke() {
        // 异步调用
        transferService.transfer(A, B, 100)
                .thenRun(() -> System.out.println("转账完成！"));
    }
}
```

---

### 实现高性能的异步网络传输

传统的同步网络IO，使用一个线程对应一个Channel，面对大量连接的时候性能低下。而Netty是异步网络框架，基于NIO，使用少量线程处理大量连接，自动处理了线程管理、缓存管理和连接管理，用户只需要进行连接初始化和实现收发数据的业务逻辑。

```java
// 创建一组线性
EventLoopGroup group = new NioEventLoopGroup();

try{
    // 初始化Server
    ServerBootstrap serverBootstrap = new ServerBootstrap();
    serverBootstrap.group(group);
    serverBootstrap.channel(NioServerSocketChannel.class);
    serverBootstrap.localAddress(new InetSocketAddress("localhost", 9999));

    // 设置收到数据后的处理的Handler,MyHandler需要自己实现
    serverBootstrap.childHandler(new ChannelInitializer<SocketChannel>() {
        protected void initChannel(SocketChannel socketChannel) throws Exception {
            socketChannel.pipeline().addLast(new MyHandler());
        }
    });
    // 绑定端口，开始提供服务
    ChannelFuture channelFuture = serverBootstrap.bind().sync();
    channelFuture.channel().closeFuture().sync();
} catch(Exception e){
    e.printStackTrace();
} finally {
    group.shutdownGracefully().sync();
}
```

---

### 序列化和反序列化

选择序列化方式思考的问题：

1. 是否方便人类阅读
2. 序列化后占用空间较小
3. 序列化速度快
4. 序列化的复杂度较低

这几个问题1和2是相互矛盾的，3和4是相互矛盾的，需要根据业务逻辑进行合理的选择

使用比较多的序列化格式是JSON、XML标准的数据格式（可读性好）或者Google 的 Protobuf、Kryo、Hessian 等（性能比JSON等好，但比专用的序列化方式差）

专用的序列化方式数据量最小，传输最快，但是需要为每个类设计序列化实现，很麻烦，大部分情况下是不考虑的

```java
//JSON序列化
byte [] serializedUser = JsonConvert.SerializeObject(user).getBytes("UTF-8");
//Kryo序列化
kryo.register(User.class);
Output output = new Output(new FileOutputStream("file.bin"));
kryo.writeObject(output, user);
//专用的序列化方式
03   | 08 7a 68 61 6e 67 73 61 6e | 17 | 01
User |    z  h  a  n  g  s  a  n  | 23 | true
第一个字节为对象的表示，第二个字节为name的长度...
```



---

### 传输协议

传输协议就是规定了传输数据的规则，使得应用程序能够识别传输的数据

实现高性能的网络通信协议，实现高性能的进程间通信：

1. 解决断句问题，一般使用前置长度；
2. 双工通信；
3. 配合专用的数据序列化方式

---

### 内存管理

优化代码中处理业务的逻辑，避免创建很多一次性使用的对象，尤其是大对象；

使用对象池循环使用对象

---

### Kafka的高性能

1. 批量处理
2. 顺序读写磁盘，提高磁盘IO。对于磁盘来说，顺序读写相比随机读写要快。对于每一个Partition都对应一个log文件，对于接收到的消息发送到对应log文件中，如果文件满了就写到新的文件中
3. 利用PageCache加速消息读写。当消息消费的时候，如果PageCache有消息，那么可以直接发送给Consumer，如果没有，那么需要到磁盘中读取。
4. 零拷贝技术。消息从磁盘消费的过程存在3次的文件复制（磁盘到PageCache，PageCache到应用程序的内存空间，再到Socket缓冲区），而我们可以直接减少1次复制，即PageCache直接到Socket缓冲区。对于文件从磁盘读取然后通过网络发送的场景，而且数据不需要进行额外处理，使用零拷贝技术可以有效提升性能

---

### 缓存策略：如何使用缓存来减少磁盘IO？

内存的速度一般是磁盘速度的十万倍，利用缓存来减少磁盘IO方式有两种：读写缓存和只读缓存

#### 读写缓存

数据的更新操作提交给缓存，之后由操作系统异步将缓存中的数据更新到磁盘，存在数据丢失的问题，但是可以通过多副本来保证可靠性。

写缓存的实现比较复杂，而Kafka使用PageCache，PageCache的读写缓存是由操作系统实现了的，Kafka只要正确使用就可以了

对于队列来说，生产和消费的速度一般是1:1，需要同时有较好的读取和写入能力，适合用读写缓存

#### 只读缓存

数据的更新操作直接更新磁盘，缓存中存在旧数据，缓存中数据更新的策略有三个：

1. 磁盘数据更新后同步更新缓存；
2. 定期全量更新缓存；

3. 缓存数据设置有效期，数据失去有效期后需要到磁盘读取数据

以上3个数据的实时性依次降低，第一个适用于金融场景

只读缓存适合读多写少的场景



---

### 锁

在并发场景下，共享资源不能提供并发访问，那么才使用锁

1. 能不用锁就不要用
2. 牢记要释放锁
3. 尽量使用可重入锁
4. 一个线程尽量不要获取多个锁，如果有多个锁，需要注意加解锁的顺序，比如获取锁A，B，C，解锁的顺序是C，B，A
5. 给所有的锁排个顺序，线程按照顺序获取锁

---

### 硬件同步原语

CAS（Compare And Swap）：比较后交换，比较通用的原语

FAA（Fetch And Add）：获取旧值odd，将其加上一个数ins，函数返回旧值odd。只能用于数值的增减，通用性较弱

能够保证在并发场景下，多线程并发访问共享资源，在线程之间竞争不是很激烈的情况下，能够替换锁。

但是如果并发数很多，会导致线程不断重试，耗费额外的CPU时间，性能反而不高。可以在每次循环结束前调用Yield，使线程让出CPU给其他线程使用

---

### 数据压缩

牺牲时间换取存储空间和网络吞吐量，解压缩需要耗费很多的CPU时间

- 压缩算法（ZIP、GZIP、Snappy、LZ4）

- 压缩段大小：大的压缩段能够有更好的压缩率，增长到一定程度就可以，不过大的压缩段，在解压缩的时候，会有更多的解压浪费，比如压缩了一个100M的文件，但是我们只需要里面一Kb的数据，那么还是需要将100M解压

对于Kafka，如果Producer和Consumer的CPU资源不是很吃紧，那么开启压缩，能够减少网络带宽和服务器的存储资源，很好得提高系统的吞吐量



### 消息复制

消息队列需要具备高性能、高可用和数据一致性（数据不丢失，消息严格顺序）

需要写入的节点数量越多，集群的可用性和数据可靠性就越好，但是性能就越差

#### RocketMQ

同步复制：数据需要写入到从节点才返回确认

异步复制：数据写入到主节点就返回确认，此时当主节点宕机的时候，数据并不会丢失，因为主从节点通过配置固定住，不支持动态变更，当主节点宕机的时候，生产者就不能产生消息，而消费者可以切换到从节点消费消息，等待主节点恢复后进行主从数据一致化。这儿牺牲了可用性，保证了高性能和数据可靠性

为了高可用性，异步复制只用一对主从节点，那么使用多对主从节点就可以有高可用性，把一个Topic分布到多个主从节点上，一对主从节点不行了，就使用另一对。但是这样不能保证数据的严格顺序，所以严格顺序和高可用是个矛盾

RocketMQ引入了Dledger。Dledger 在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客户端返回写入成功，并且它是支持通过选举来动态切换主节点的。

- 传统的主从模式（同步复制，异步复制）：性能好，可用性和灵活性较差
- Dledger：可用性好，灵活性好，性能较差

#### Kafka

Kafka中基本复制单位是分区，每个分区有多个副本，分区的多个副本采用一主多从的方式。Kafka 在写入消息的时候，采用的也是异步复制的方式。消息在写入到主节点之后，并不会马上返回写入成功，而是等待足够多的节点都复制成功后再返回。在 Kafka 中这个“足够多”是多少呢？Kafka 的设计哲学是，让用户自己来决定。

Kafka使用Zookeeper来进行选主



### NameServer

作用：

1. 帮助客户端寻找Topic对应的Broker信息。客户端在启动后，会定时向NameServer获取主体相关的路由信息，缓存在本地内存，在需要的时候使用
2. 监控集群中的Broker状态。Broker会定期向NameServer上报路由信息，同时具备心跳的功能，当NameServer超时没有收到Broker的信息则认为Broker宕机，会让客户端选择其他可用的Broker

NameServer几个主要的类：

- NamesrvStartup：程序入口。

- NamesrvController：NameServer 的总控制器，负责所有服务的生命周期管理。

- RouteInfoManager：NameServer 最核心的实现类，负责保存和管理集群路由信息。

- BrokerHousekeepingService：监控 Broker 连接状态的代理类。

- DefaultRequestProcessor：负责处理客户端和 Broker 发送过来的 RPC 请求的处理器。

- ClusterTestRequestProcessor：用于测试的请求处理器。



### ZooKeeper

ZooKeeper的作用：

1. 集群选主，节点监控；
2. 分布式锁；
3. 小规模的KV存储

Kafka 主要使用 ZooKeeper 来保存它的元数据、监控 Broker 和分区的存活状态，并利用 ZooKeeper 来进行选举。作用类似于RocketMQ使用NameServer。

Kafka 的客户端并不会去直接连接 ZooKeeper，它只会和 Broker 进行远程通信，ZooKeeper 上的元数据是通过 Broker 中转给每个客户端的。Broker中保存了一份和ZooKeeper一样的元数据，由于ZooKeeper的Watcher机制，ZooKeeper不需要每次客户端请求的时候都去ZooKeeper获取元数据，只需要返回Broker中的元数据即可，当元数据有变更的时候ZooKeeper会自动更新并且发送给Broker



### MQTT

MQTT是在物联网环境中使用的标准消息队列通信协议

需求：海量客户端和主题（千万级），每个物体就是一个主题，物体订阅获取主题相关的消息；物体的计算和存储单元很小，协议需要足够简介且高效

一个MQTT集群架构如下所示：![](F:\MyGitHub\my\MessageQueue\pic\MQTT集群架构.jpg)

Proxy集群有三个作用：处理海量连接；中转Brokers的消息；维护和客户端的会话

Brokers集群负责保存和收发消息

Proxy集群对会话的处理可以有两种：

- 将会话保存在Proxy集群中，每个Proxy处理部分的客户端，需要LB具备Sticky Session来保证客户端总是由同一个Proxy处理
- 将会话保存在外置存储集群中，比如Redis、MySQL等，这样Proxy可以设置为无状态，LB也没有要求，需要外置存储集群有较大的存储容量和性能



### Pulsar

通常的消息队列集群结构，消息都是存放在Broker集群中，而Pulsar的Broker集群不保存消息数据和元数据，消息保存在Bookie（Bookeeper集群）中，Bookeeper类似于HDFS，是一个分布式存储系统。元数据保存在ZooKeeper中，Broker是无状态的，任何一个Broker宕机，都可以立即使用另外一个Broker

BookKeeper 的存储单元是 Ledger，Ledger 就是一段 WAL（Write Ahead Log），它包含了连续的若干条消息，消息在 Ledger 中称为 Entry。为了保证 Ledger 中的 Entry 的严格顺序，Pulsar 为 Ledger 增加一次性的写入限制，Broker 创建一个 Ledger 后，只有这个 Broker 可以往 Ledger 中写入 Entry，一旦 Ledger 关闭后，无论是 Broker 主动关闭，还是因为 Broker 宕机异常关闭，这个 Ledger 就永远只能读取不能写入了。如果需要继续写入 Entry，只能新建另外一个 Ledger。

这种“一次性写入”的机制，能够解决并发写入控制的问题，不然对于共享数据的并发读写需要设计分布式锁来保证数据一致性

![](F:\MyGitHub\my\MessageQueue\pic\Pulsar架构.png)

优点：

1. 存储和计算分离，对于计算节点的开发者来说，只需要关注计算业务逻辑，而不需要考虑数据一致性，故障转移，数据读写性能，数据可靠性等存储问题。
2. Broker节点是无状态的，无状态的集群更容易管理和维护，天然支持水平扩展，负载均衡可以很灵活，故障转移也很简单

3. 对于存储节点的开发，只需要考虑如何高效地存储数据即可，而且可以直接使用成熟的存储系统，比如HDFS，MySQL等。

